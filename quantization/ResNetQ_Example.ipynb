{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jesusnavarro/Desktop/cs282_proj\n"
     ]
    }
   ],
   "source": [
    "%cd ..\n",
    "from resnet_quant import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " PreAct_ResNet_Cifar_Q(\n",
      "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): PreActBasicBlock_convQ(\n",
      "      (act_qfn): activation_quantize_fn()\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv1): Conv2d_Q(\n",
      "        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (qfn): weight_quantize_fn()\n",
      "      )\n",
      "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d_Q(\n",
      "        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (qfn): weight_quantize_fn()\n",
      "      )\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): PreActBasicBlock_convQ(\n",
      "      (act_qfn): activation_quantize_fn()\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv1): Conv2d_Q(\n",
      "        16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
      "        (qfn): weight_quantize_fn()\n",
      "      )\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d_Q(\n",
      "        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (qfn): weight_quantize_fn()\n",
      "      )\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "      )\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): PreActBasicBlock_convQ(\n",
      "      (act_qfn): activation_quantize_fn()\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv1): Conv2d_Q(\n",
      "        32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
      "        (qfn): weight_quantize_fn()\n",
      "      )\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d_Q(\n",
      "        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (qfn): weight_quantize_fn()\n",
      "      )\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "      )\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (avgpool): AvgPool2d(kernel_size=8, stride=1, padding=0)\n",
      "  (fc): Linear(in_features=64, out_features=100, bias=True)\n",
      ")\n",
      "conv1 Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "layer1 Sequential(\n",
      "  (0): PreActBasicBlock_convQ(\n",
      "    (act_qfn): activation_quantize_fn()\n",
      "    (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv1): Conv2d_Q(\n",
      "      16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (qfn): weight_quantize_fn()\n",
      "    )\n",
      "    (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d_Q(\n",
      "      16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (qfn): weight_quantize_fn()\n",
      "    )\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      ")\n",
      "layer1.0 PreActBasicBlock_convQ(\n",
      "  (act_qfn): activation_quantize_fn()\n",
      "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv1): Conv2d_Q(\n",
      "    16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "    (qfn): weight_quantize_fn()\n",
      "  )\n",
      "  (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv2): Conv2d_Q(\n",
      "    16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "    (qfn): weight_quantize_fn()\n",
      "  )\n",
      "  (relu): ReLU(inplace=True)\n",
      ")\n",
      "layer1.0.act_qfn activation_quantize_fn()\n",
      "layer1.0.bn1 BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "layer1.0.conv1 Conv2d_Q(\n",
      "  16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "  (qfn): weight_quantize_fn()\n",
      ")\n",
      "layer1.0.conv1.qfn weight_quantize_fn()\n",
      "layer1.0.bn2 BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "layer1.0.conv2 Conv2d_Q(\n",
      "  16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "  (qfn): weight_quantize_fn()\n",
      ")\n",
      "layer1.0.conv2.qfn weight_quantize_fn()\n",
      "layer1.0.relu ReLU(inplace=True)\n",
      "layer2 Sequential(\n",
      "  (0): PreActBasicBlock_convQ(\n",
      "    (act_qfn): activation_quantize_fn()\n",
      "    (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv1): Conv2d_Q(\n",
      "      16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
      "      (qfn): weight_quantize_fn()\n",
      "    )\n",
      "    (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d_Q(\n",
      "      32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (qfn): weight_quantize_fn()\n",
      "    )\n",
      "    (downsample): Sequential(\n",
      "      (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "    )\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      ")\n",
      "layer2.0 PreActBasicBlock_convQ(\n",
      "  (act_qfn): activation_quantize_fn()\n",
      "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv1): Conv2d_Q(\n",
      "    16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
      "    (qfn): weight_quantize_fn()\n",
      "  )\n",
      "  (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv2): Conv2d_Q(\n",
      "    32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "    (qfn): weight_quantize_fn()\n",
      "  )\n",
      "  (downsample): Sequential(\n",
      "    (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "  )\n",
      "  (relu): ReLU(inplace=True)\n",
      ")\n",
      "layer2.0.act_qfn activation_quantize_fn()\n",
      "layer2.0.bn1 BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "layer2.0.conv1 Conv2d_Q(\n",
      "  16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
      "  (qfn): weight_quantize_fn()\n",
      ")\n",
      "layer2.0.conv1.qfn weight_quantize_fn()\n",
      "layer2.0.bn2 BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "layer2.0.conv2 Conv2d_Q(\n",
      "  32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "  (qfn): weight_quantize_fn()\n",
      ")\n",
      "layer2.0.conv2.qfn weight_quantize_fn()\n",
      "layer2.0.downsample Sequential(\n",
      "  (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      ")\n",
      "layer2.0.downsample.0 Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "layer2.0.relu ReLU(inplace=True)\n",
      "layer3 Sequential(\n",
      "  (0): PreActBasicBlock_convQ(\n",
      "    (act_qfn): activation_quantize_fn()\n",
      "    (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv1): Conv2d_Q(\n",
      "      32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
      "      (qfn): weight_quantize_fn()\n",
      "    )\n",
      "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d_Q(\n",
      "      64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (qfn): weight_quantize_fn()\n",
      "    )\n",
      "    (downsample): Sequential(\n",
      "      (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "    )\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      ")\n",
      "layer3.0 PreActBasicBlock_convQ(\n",
      "  (act_qfn): activation_quantize_fn()\n",
      "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv1): Conv2d_Q(\n",
      "    32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
      "    (qfn): weight_quantize_fn()\n",
      "  )\n",
      "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv2): Conv2d_Q(\n",
      "    64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "    (qfn): weight_quantize_fn()\n",
      "  )\n",
      "  (downsample): Sequential(\n",
      "    (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "  )\n",
      "  (relu): ReLU(inplace=True)\n",
      ")\n",
      "layer3.0.act_qfn activation_quantize_fn()\n",
      "layer3.0.bn1 BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "layer3.0.conv1 Conv2d_Q(\n",
      "  32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
      "  (qfn): weight_quantize_fn()\n",
      ")\n",
      "layer3.0.conv1.qfn weight_quantize_fn()\n",
      "layer3.0.bn2 BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "layer3.0.conv2 Conv2d_Q(\n",
      "  64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "  (qfn): weight_quantize_fn()\n",
      ")\n",
      "layer3.0.conv2.qfn weight_quantize_fn()\n",
      "layer3.0.downsample Sequential(\n",
      "  (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      ")\n",
      "layer3.0.downsample.0 Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "layer3.0.relu ReLU(inplace=True)\n",
      "bn BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "relu ReLU(inplace=True)\n",
      "avgpool AvgPool2d(kernel_size=8, stride=1, padding=0)\n",
      "fc Linear(in_features=64, out_features=100, bias=True)\n"
     ]
    }
   ],
   "source": [
    "# Importing model\n",
    "qparams = [4, 4, 'dorefa'] # [abit, qbit, quant_method] --- only dorefa has been implemented\n",
    "\n",
    "# resnet models are 8, 14, 16, 24\n",
    "\n",
    "# activation and weight quantization are computed slightly different.\n",
    "quant_resnet = get_quant_model('resnet8', qparams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: first and last layer are not quantized \n",
    "# looking at layer ouptuts using hooks\n",
    "outputs=[]\n",
    "quant_resnet = get_quant_model('resnet8', qparams)\n",
    "\n",
    "\n",
    "def hook(module, input, output):\n",
    "    outputs.append(output)\n",
    "\n",
    "# Registering quantized act and \n",
    "for n, m in quant_resnet.named_modules():\n",
    "    if n == 'conv1': # first layer output\n",
    "        m.register_forward_hook(hook)\n",
    "    elif 'qfn' in n:\n",
    "        m.register_forward_hook(hook)\n",
    "    elif 'fc' in n:\n",
    "        m.register_forward_hook(hook)\n",
    "import torch\n",
    "# reset model or it will continue appending outputs \n",
    "image = torch.unsqueeze(torch.rand(size=(3, 32, 32)), dim=0)\n",
    "out = quant_resnet(image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.08691858 -0.13056552 -0.07414071 ... -0.32856515 -0.5100109\n",
      " -0.31090957]\n",
      "[0.         0.         0.         ... 0.6        0.         0.73333335]\n",
      "[-0.0940421  -0.03134737  0.03134739 ... -0.03134737 -0.03134737\n",
      "  0.03134739]\n",
      "[0.6        0.         0.4        ... 0.46666667 0.         0.2       ]\n",
      "[-0.03089878  0.03089881 -0.15449388 ...  0.03089881 -0.09269633\n",
      " -0.09269633]\n",
      "[0.         0.         0.         ... 1.         0.53333336 1.        ]\n",
      "[-0.01912601 -0.13388206 -0.01912601 ... -0.01912601  0.01912603\n",
      " -0.01912601]\n",
      "[0.         0.         0.46666667 ... 0.93333334 0.         0.        ]\n",
      "[ 0.02225179  0.02225179  0.11125886 ... -0.06675531  0.02225179\n",
      "  0.1557624 ]\n",
      "[0. 1. 0. ... 0. 0. 0.]\n",
      "[ 0.04640684  0.1082826   0.04640684 ...  0.07734472  0.04640684\n",
      " -0.01546894]\n",
      "[0.         0.         0.26666668 ... 0.         0.06666667 0.8666667 ]\n",
      "[ 0.01505579  0.04516736  0.07527892 ... -0.0752789  -0.04516734\n",
      " -0.01505578]\n",
      "[-0.08671793  0.18297285 -0.19683298  0.16536272 -0.30727726 -0.1950524\n",
      " -0.29528025  0.4667179  -0.10643439  0.01313841  0.31263784  0.01782516\n",
      " -0.16513713  0.09794978 -0.05964646  0.26092908 -0.12210678 -0.2472537\n",
      " -0.0357831  -0.26637352  0.00943197  0.2277826  -0.4478019  -0.01228225\n",
      "  0.12346648  0.11135715 -0.14721732 -0.11303873 -0.37008354  0.15620747\n",
      "  0.05191626  0.53728503  0.2768386  -0.08877824 -0.19267477  0.3321988\n",
      " -0.0053199   0.3665939   0.1386917  -0.2941875  -0.26368365 -0.05897401\n",
      "  0.3430804  -0.04940917  0.14153542  0.31787014  0.42343897  0.17135699\n",
      " -0.06481424 -0.21407013 -0.05603826 -0.09003343 -0.17275773  0.02452034\n",
      "  0.28418452  0.32471102  0.02745584 -0.32914168 -0.3653317   0.3920741\n",
      " -0.20500734 -0.03448029  0.20084262 -0.27729195 -0.12489603 -0.22804019\n",
      "  0.22173923  0.16904302 -0.06891249 -0.14115931 -0.07914953  0.06717013\n",
      "  0.03483588  0.19501323 -0.36756504 -0.26885897  0.14505811 -0.19697683\n",
      "  0.5396188  -0.20939048 -0.23765129 -0.25072396 -0.47658762 -0.43287447\n",
      "  0.19298318  0.25357628  0.32717624 -0.06437513 -0.32638654  0.26390037\n",
      " -0.3357768  -0.20092161  0.10675271 -0.26860258  0.23051102  0.58568203\n",
      "  0.09431086  0.3137732   0.00949699  0.01921892]\n"
     ]
    }
   ],
   "source": [
    "# look at some values at intermediate layer outputs\n",
    "# weights and acitvations are quantized but operations are still in full precision\n",
    "# with the final layer also full precision\n",
    "\n",
    "for output in outputs:\n",
    "    output = output.view(1, -1)\n",
    "    print(np.squeeze(output.data.numpy()[0:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# print outputs\n",
    "# simple loss function to look at gradients\n",
    "loss = torch.sum(out)\n",
    "print(out.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
