{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Range-Based Linear Quantization\n",
    "float is quantized linearly using a scale factor\n",
    "#### 1. Asymmetric:\n",
    "- Scale factor is computed using the min/max of both float/input and quantized type. A zero_point is introduced (bias)\n",
    "    \n",
    "$$ x_q = round\\Big(x_f * \\frac{2^k - 1}{max(x_f) - min(x_f)} - zp\\Big) \\\\\n",
    " zp = round\\Big(min\\big(x_f, \\frac{2^k - 1}{max(x_f) - min(x_f)}\\big)\\Big) \\\\ $$\n",
    "\n",
    "- quantized outputs of FC or convs can be computed by plugging in full precision weights, biases,  and inputs as a function of quantized values\n",
    "-  gemmlowp documentation: https://github.com/google/gemmlowp/blob/master/doc/quantization.md#implementation-of-quantized-matrix-multiplication\n",
    "\n",
    "#### 2. Symmetric\n",
    "- Instead of using min/max of float range, we use $ [-max(abs(x_f)), max(abs(x_f))] $\n",
    "\n",
    "- No zero point, so range is symmetric about 0 for both float and quantized range\n",
    "\n",
    "$$ x_q = round\\Big(x_f * \\frac{2^k - 1}{max(abs(x_f))}\\Big) $$\n",
    "\n",
    "#### Tradeoffs:\n",
    "- In symmetric approach, if float range is biased, a protion of quantized range may be dedicated to a range of float values we may not see\n",
    "- Implementing symmetric approach is simpler\n",
    "\n",
    "#### Other features:\n",
    "- removing outliers and scale factor approximation (post-training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DoReFa\n",
    "paper: https://arxiv.org/abs/1606.06160\n",
    "\n",
    "- defines a function 'quantize_k()' which takes in a real value $a_f \\in [0,1]$ and outputs a discrete-valued $a_k \\in [\\frac{0}{2^k - 1}, \\frac{1}{2^k - 1}, ..., \\frac{2^k - 1}{2^k - 1}] $ where k is the bit precision\n",
    "\n",
    "$$ x_q = quantize(x_f) = \\frac{1}{2^k - 1} round((2^k - 1) x_f) $$\n",
    "\n",
    "#### Activations:\n",
    "- Activations are clipped [0,1], then applied the quantized function\n",
    "\n",
    "\n",
    "#### Weights:\n",
    "- Weights are applied a function $f(w)$, then quantized using the quantize function.\n",
    "    \n",
    "$$ f(w) = \\frac{tanh(w)}{2max(|tanh(w)|)} + 0.5 \\\\\n",
    "   w_q = 2*quantize(f(w)) - 1 $$\n",
    "    \n",
    "#### Notes:\n",
    "- requires quantization-aware training (https://nervanasystems.github.io/distiller/quantization.html#quantization-aware-training)\n",
    "- graident quantization discusses but not supported on Distiller\n",
    "- binary quantization not supported"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PACT: Parameterized Clipping Activation for Quantized Neural Networks)\n",
    "paper: https://arxiv.org/abs/1709.01134\n",
    "- Similar to DoReFa, but the ppper clipping values for the activations $ \\alpha $ are learned and not hard coded to 1\n",
    "\n",
    "\n",
    "# WRPN: Wide Reduced-Precision Networks)\n",
    "- Similar to DoReFa \n",
    "- activations clipped [0, 1] while weights are clipped [-1, 1]\n",
    "- quantization is done with k-1 bits to allow one bit for sign\n",
    "- Paper discusses using wider layers to increase accuracy.\n",
    "- Wider layers and binary weights not supported in Distiller"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Quantized Tensors\n",
    "- operator implementations only support channel-quantization for weights of conv and linear operators\n",
    "- min and max of input data is linearly mapped to min/max of output data type\n",
    "- documentation: https://pytorch.org/docs/stable/quantization.html\n",
    "\n",
    "##### Mapping:\n",
    "$$ Q(x, s, b) = round( \\frac{x}{s} +  b)$$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.1340, -0.2858, -0.6741, -0.5410])\n",
      "tensor([ 0.4980, -0.2863, -0.5020, -0.5020], size=(4,), dtype=torch.qint8,\n",
      "       quantization_scheme=torch.per_tensor_affine, scale=0.00392156862745098,\n",
      "       zero_point=0)\n"
     ]
    }
   ],
   "source": [
    "# Quantize per tensor\n",
    "k = 8\n",
    "scale = (2**k - 1)**(-1)\n",
    "zero_point = 0\n",
    "a = torch.tensor(np.random.randn(4).astype('f'), dtype=torch.float32)\n",
    "b = torch.quantize_per_tensor(a, scale=scale, zero_point=zero_point, dtype=torch.qint8)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
