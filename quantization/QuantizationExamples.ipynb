{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Range-Based Linear Quantization\n",
    "float is quantized linearly using a scale factor\n",
    "#### 1. Asymmetric:\n",
    "- Scale factor is computed using the min/max of both float/input and quantized type. A zero_point is introduced (bias)\n",
    "    \n",
    "$$ x_q = round\\Big(x_f * \\frac{2^k - 1}{max(x_f) - min(x_f)} - zp\\Big) \\\\\n",
    " zp = round\\Big(min\\big(x_f, \\frac{2^k - 1}{max(x_f) - min(x_f)}\\big)\\Big) \\\\ $$\n",
    "\n",
    "- quantized outputs of FC or convs can be computed by plugging in full precision weights, biases,  and inputs as a function of quantized values\n",
    "-  gemmlowp documentation: https://github.com/google/gemmlowp/blob/master/doc/quantization.md#implementation-of-quantized-matrix-multiplication\n",
    "\n",
    "#### 2. Symmetric\n",
    "- Instead of using min/max of float range, we use $ [-|max(abs(x_f))|, |max(abs(x_f))|] $\n",
    "\n",
    "- No zero point, so range is symmetric about 0 for both float and quantized range\n",
    "\n",
    "$$ x_q = round\\Big(x_f * \\frac{2^k - 1}{max(abs(x_f))}\\Big) $$\n",
    "\n",
    "#### Tradeoffs:\n",
    "- In symmetric approach, if float range is biased, a protion of quantized range may be dedicated to a range of float values we may not see\n",
    "- Implementing symmetric approach is simpler\n",
    "\n",
    "#### Other features:\n",
    "- removing outliers and scale factor approximation (post-training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DoReFa\n",
    "paper: https://arxiv.org/abs/1606.06160\n",
    "\n",
    "- defines a function 'quantize_k()' which takes in a real value $a_f \\in [0,1]$ and outputs a discrete-valued $a_k \\in [\\frac{0}{2^k - 1}, \\frac{1}{2^k - 1}, ..., \\frac{2^k - 1}{2^k - 1}] $ where k is the bit precision\n",
    "\n",
    "$$ x_q = quantize(x_f) = \\frac{1}{2^k - 1} round((2^k - 1) x_f) $$\n",
    "\n",
    "#### Activations:\n",
    "- Activations are clipped [0,1], then applied the quantized function\n",
    "\n",
    "\n",
    "#### Weights:\n",
    "- Weights are applied a function $f(w)$, then quantized using the quantize function.\n",
    "    \n",
    "$$ f(w) = \\frac{tanh(w)}{2max(|tanh(w)|)} + 0.5 \\\\\n",
    "   w_q = 2*quantize(f(w)) - 1 $$\n",
    "    \n",
    "#### Notes:\n",
    "- requires quantization-aware training (https://nervanasystems.github.io/distiller/quantization.html#quantization-aware-training)\n",
    "- graident quantization discusses but not supported on Distiller\n",
    "- binary quantization not supported"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PACT: Parameterized Clipping Activation for Quantized Neural Networks)\n",
    "paper: https://arxiv.org/abs/1709.01134\n",
    "- Similar to DoReFa, but the ppper clipping values for the activations $ \\alpha $ are learned and not hard coded to 1\n",
    "\n",
    "\n",
    "# WRPN: Wide Reduced-Precision Networks)\n",
    "- Similar to DoReFa \n",
    "- activations clipped [0, 1] while weights are clipped [-1, 1]\n",
    "- quantization is done with k-1 bits to allow one bit for sign\n",
    "- Paper discusses using wider layers to increase accuracy.\n",
    "- Wider layers and binary weights not supported in Distiller"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Quantized Tensors\n",
    "- operator implementations only support channel-quantization for weights of conv and linear operators\n",
    "- min and max of input data is linearly mapped to min/max of output data type\n",
    "- documentation: https://pytorch.org/docs/stable/quantization.html\n",
    "\n",
    "##### Mapping:\n",
    "$$ Q(x, s, b) = round( \\frac{x}{s} +  b)$$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "quantize only works on Float Tensor.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-65e0189fb3c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mzero_point\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'f'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantize_per_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzero_point\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mzero_point\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: quantize only works on Float Tensor."
     ]
    }
   ],
   "source": [
    "# Quantize per tensor\n",
    "k = 8\n",
    "scale = (2**k - 1)**(-1)\n",
    "zero_point = 0\n",
    "a = torch.tensor(np.random.randn(4).astype('f'), dtype=torch.float64)\n",
    "b = torch.quantize_per_tensor(a, scale=scale, zero_point=zero_point, dtype=torch.uint8)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
